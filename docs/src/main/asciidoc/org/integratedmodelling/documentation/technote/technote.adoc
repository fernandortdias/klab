= A semantic web platform for science
Technical note
v1.0, 2021-02-20
:doctype: article
:description: High-level technical description of k.LAB for technical partners
:kl: k.LAB
:kmod: k.Modeler
:kact: k.Actors
:keng: k.LAB Engine
:knod: k.LAB Node
:kim: k.IM
:ked: k.LAB Resource Editor
:pex: k.LAB Explorer
:kex: Project Explorer
:encoding: utf-8
:lang: en
:title-page:
:toc: left
:toclevels: 5
:sectnums:
:sectnumlevels: 5
:numbered:
:experimental:
:reproducible:
:icons: font
:listing-caption: Listing
:sectnums:
:autofit-option:
:mdash: &#8212;
:language: asciidoc
ifdef::backend-pdf[]
:title-logo-image: image:resources_handling/imgs/KLAB_LOGO.png[align=center]
:source-highlighter: rouge
//:rouge-style: github
//:source-highlighter: pygments
//:pygments-style: tango
endif::[]
:stem:

<<<

*Integrated modeling* is a practice meant to maximize the value of scientific information by ensuring its  _modularity_, _reusability_, _interoperability_ and _traceability_ throughout the scientific process. The k.LAB software, discussed here, is a full-stack solution for integrated modelling, supporting the production, curation, linking and deployment of scientific artifacts such as datasets, data services, modular model components and distributed computational services. The purpose of k.LAB is to ensure -- by _design_ rather than intention -- that the pool of such artifacts constitutes a seamless _knowledge commons_, readily actionable (by humans or machines) through the full realization of the _linked data_ paradigm [REF] augmented with semantics and powered by machine reasoning. This design enables automation of a wide range of modeling tasks that were previously only performable by experts and on an ad-hoc basis.

The {kl} platform directly addresses the four FAIR goals (Findable, Accessible, Interoperable and Reusable), introducing innovations particularly in the practice of semantic annotation,  which is reviewed into a modern, expressive approach meant to ease adoption by providers and users. To the four dimensions in FAIR, {kl} adds a _reactivity_ dimension that enables knowledge to also be _deployed_ in an _"internet of observations"_, creating _live_ artifacts that can evolve, improve and perform actions as new information appears on the network.

The central service in the k.LAB modeling API receives as input a logical query of the form "observe _concept_ in _context_" (e.g., "observe _change in land cover type_ in _Colombia, 2015-2020_", only slightly paraphrased from k.LAB's near-natural query formalism) and, in response, assembles, documents, initializes and runs a computation (called a _dataflow_) that produces the  *observation* of the concept that best fits the context, based on the integration of data and model components available in the k.LAB network. The observations output by the API request, along with the dataflow assembled to generate them, are themselves scientific artifacts -- automatically augmented with provenance records and user-readable documentation -- that can be exported and stored as needed. 

Artificial intelligence, driven by both semantics (_machine reasoning_) and the analysis of previous outcomes (_machine learning_), satisfies the request using a shared, communally owned and curated knowledge base (the _worldview_, a set of ontologies) and the resource pool available at any given moment on the k.LAB network, by ranking, selecting, adapting, and connecting data and model components made available by independent and uncoordinated providers.

This document is a brief sketch of the {kl} main principles and architecture. Detailed documentation for {kl} is in development and is referenced where available.

## Architecture of the k.LAB platform

The open source k.LAB software stack includes five components that support the creation, maintenance and use of a distributed _semantic web platform_ where scientific information can be stored, published, curated and connected. The software is licensed through the Affero General Public License (AGPL) v.3.0 and is available for the most part at the https://bitbucket.org/integratedmodelling/klab[{kl} git repository].

### The software stack

* *Server* components are deployed by certified _partners_ to publish resources and semantic content (*{knod}*) and/or provide modeling services and applications (*{keng}*) to online users. Published resources can include both static data and dynamic computations, both of which may be hosted in source form at the node or linked to external data (e.g. WCS, WFS, OpenDAP) or computational services (e.g. OpenCPU). The k.Node software is deployed in containers that can be configured to host dedicated instances of Geoserver, PostgreSQL, Hyrax or other services; these are transparently managed through server adapters inside the node, eliminating the need for alphabetization of node administrators.
* *Client* components are used by contributors to develop, validate and publish resources and semantic content (*k.Modeler*, an Integrated Development Environment (IDE) for semantic modeling), and by end users (*k.Explorer*) to access modeling services and specialized applications built for the platform and delivered through the web. 

Additional server components serve specific needs on the {kl} network. Among them the following are noteworthy:

* The _hub_ server, *k.Hub*, manages authentication and organizes node access for authenticated engines. The Integrated Modeling Partnership manages a set of nodes and a main hub, and releases site certificates that enable nodes to be connected to form the platform.Partners that need to manage users locally may also deploy and connect a hub, although this is not normally required except in large deployments.
* A _semantic server_ collects and indexes the semantic knowledge from the worldview and all public projects, constantly compiling and revising documentation and use cases to assist users in the semantic annotation process. Users can look up annotations made by others and access hyperlinked, evolving descriptions of each concept and predicate, with on-the-fly logical validation of logical expressions in models being developed and a suggestion service that can find and propose comparisons with use cases extracted from peer-reviewed public projects. Through the use of specialized metadata inserted in {kim} source files, the server can be integrated with the editors so that assistance is available directly, to ease the development of semantic content as much as possible. The semantic server is in development and is not available to the general public yet.

**Usage configurations**: [DISCUSS END-USER (+k.Apps) vs MODELER vs SITE ADMINISTRATOR] The k.LAB engine, a server-side component, can also be run at the client side in a local configuration, so that new content can be developed and tested in a sandboxed environment before publishing, with full access to public resources. Such client use is supported and facilitated by a small, downloadable https://integratedmodelling.org/get_started[control center application] that removes the complexities linked to installing, upgrading, starting and stopping the engine or the IDE. The k.LAB distributed paradigm supports and enforces a model where information remains under the ownership of its authoritative sources while maximizing its availability and interoperability, compatibly with both public and commercial services thanks to careful attribution and to state-of-the-art encryption, access control and security.

### The {kl} logical layers

The set of active, connected nodes and engines at any given time forms what can be seen collectively as a distributed container, where scientific knowledge is found in **three  layers** handling information at increasing levels of abstraction: the _resources_, _semantic_ and _reactivity_ layers. The first can be seen as a data curation platform based on modern linked data concepts and optimized for semantic annotation and deployment. Semantic and reactive content for the platform is developed in the respective layers using two specialized languages, _{kim}_ for semantic resources and _{kact}_ for rective behaviors and applications. The modeler IDE (_{kmod}_) provides drag-and-drop interfaces to build resources and specialized editors for {kl} projects, containing both {kim} and {kact} resources.

* The *resources* layer provides a _protocol_ for conventional data and computational resources or services to interoperate at the data level, matching identifiers, data types and metadata through a uniform API. Nodes and client applications include interfaces to manage development and submission of knowledge to the resources layer, to be published and curated either on-site or through hosting providers with full control of licensing and access. 
* The *semantic* layer provides a _language_ that enables annotating resources through semantically explicit logical expressions, ensuring findability, interoperability and accessibility through purely logical queries, validating consistency and producing mediation strategies through machine reasoning and logical inference. The semantic layer uses the **{kim}** language to specify semantic knowledge (compatible with W3-endorsed https://www.w3.org/TR/owl-guide/[OWL 2]) and models; these specifications, collected into namespaces and projects, can be deployed to {knod}s for the {kl} inference engines to find, rank and use.
* The *reactivity* layer provides _behaviors_ for the scientific artifacts produced by running queries in the semantic layer. Reactive observations exist in {keng}s and can react to each other either locally (within the same engine) or remotely. The reactivity layer enables distributed agent-based simulations and computations that automatically adapt to changing conditions or states. The **{kact}** language is used to define behaviors for the reactivity layer. As a special case, behaviors bound to users and sessions can be used to quickly develop specialized interactive applications that run on the platform through web browsers.

The full separation of concerns and APIs in the three layers maximizes their value: for example, the resources layer can be seen through different worldviews, therefore serving different purposes in different networks by reinterpreting it through the logical "lens" of a differently configured semantic layer.

## Resources 

REMOVE ALL THE SHIT BELOW; JUST USE EXAMPLES FOR EACH LAYER

## Semantic modeling

Semantic layer must show concepts, models (also learn) and describe resolution and dataflows. Needs pictures.

### The worldview

#### Authorities

### Models

### Machine learning

### Resolution and contextualization

## Behaviors and applications

### Applications

## Current status