= k.LAB: a semantic web platform for integrated science.
Technical note, June 2020
Ferdinando Villa, Ph.D.
:doctype: book
:encoding: utf-8
:lang: en
:toc: left
:numbered:


*Integrated modeling* is a practice meant to maximize the value of scientific information by ensuring its  _modularity_, _reusability_, _interoperability_ and _traceability_ throughout the scientific process. The k.LAB software, discussed here, is a full-stack solution for integrated modelling, supporting the production, curation and deployment of scientific artifacts such as datasets, modular model components and distributed computational services. The purpose of k.LAB is to ensure _by design_ that the pool of artifacts constitutes a _knowledge commons_, actuated through the full realization of the _linked data_ paradigm [REF] and the use of semantics to automate a wide range of modeling tasks that were previously only performable by experts.

The central service in the k.LAB modeling API receives as input a logical query of the form `observe` _concept_ `in` _context_ (e.g., `observe` _change in land cover type_ `in` _Colombia, 2015-2020_, slightly paraphrased from k.LAB's near-natural query formalism) and, in response, assembles, documents and runs a computation (called a _dataflow_) that produces the  *observation* of the concept that best fits the context, based on the integration of data and model components available in the k.LAB network. The observation output by the API request is a new scientific artifact -- augmented with provenance records and user-readable documentation -- that can be exported or curated for sharing through the same network or any other service. Artificial intelligence, driven by both semantics (_machine reasoning_) and the analysis of previous outcomes (_machine learning_), satisfies the request using a shared knowledge base (the _worldview_, a set of ontologies) and the resources available on the k.LAB network, by ranking, selecting, adapting, and connecting data and model components made available by independent and uncoordinated providers.

## Architecture of the k.LAB platform

The open source k.LAB software stack includes five components that support the creation, maintenance and use of a distributed _semantic web platform_ where scientific information can be stored, published, curated and connected. The platform directly addresses the four FAIR goals (Findable, Accessible, Interoperable and Reusable), to which it adds a _reactivity_ dimension that enables knowledge to also be _deployed_ in an _"internet of observations"_, creating _live_ artifacts that can change and improve as new information appears on the network. 

. *Server* components (*k.Node*) are deployed by certified _partners_ to publish resources and semantic content (*k.Server*) and/or provide semantic modeling services (*k.Engine*) to online users. One k.Node instance is installed at a partner site, where it can be configured (through its administration interface) to spawn as many k.Server and/or k.Engine instances as desired, using Docker technology. Published content includes both static data and dynamic computations; it may be hosted natively on k.Servers or linked from external data services (e.g. OGC, OpenDAP), or from computational services (e.g. OpenCPU). The k.Server containers managed automatically by k.Node can be configured to host dedicated instances of Geoserver, PostgreSQL, Hyrax or other services that are transparently managed through server adapters, eliminating the need for alphabetization on behalf of node administrators.
. *Client* components are used by modelers (*k.Modeler*, an Integrated Development Environment (IDE) for semantic modeling) to develop, validate and publish resources and semantic content, and by end users (*k.Explorer*) to access modeling services with novel intuitiveness and transparency. The k.LAB engine can be run at the client side in a local configuration, so that new content can be developed and tested in a sandboxed environment before publishing, with full access to public resources. Such client use is supported and facilitated by a small, downloadable https://integratedmodelling.org/get_started[control center application] that removes any complexity linked to starting and stopping the engine or the IDE. The k.LAB distributed paradigm supports and enforces a model where information remains under the ownership of its authoritative sources while maximizing its availability and interoperability, compatibly with both public and commercial services thanks to state-of-the-art access control and security.

In addition, a *k.Hub* server component manages authentication and organizes nodes. The k.LAB platform is created by deploying k.Nodes at partner sites. The Integrated Modeling Partnership manages a set of nodes and a main hub, and releases site certificates that enable nodes to be connected to form the platform. The site certificate commits the node to a worldview and specifies its connection to a hub. Partners that need to manage users locally can also deploy and connect a hub, although this is normally not necessary. The set of active, connected nodes at a given time forms what can be seen collectively as a distributed _container_, where scientific knowledge is found in three distinct layers:

The *resource layer* linked data paradigm/ Deals with RESOURCES i.e. any content that is not semantic. Resources are developed, validated, published and curated. Information is only persisted and curated in the resource layer; there is no semantics. It is the layer where existing data produces, services, and computations are most close.

The *semantic layer* deals with Semantic resources exist at the "conceptual"  level (models, concepts), at the "physical" level (semantically annotated observations, "live" within a session) and can be exported back to resources for publication and curation. In the semantic layer, links are made between resources and their meaning: by adopting a representational - principles of orthogonality and parsimony ... The *k.IM language*, a  language optimized for semantic annotation... , is central to the semantic layer and ensures...

The *reactivity layer* connects _behaviors_ to any artifacts created during a k.LAB session, enabling individual-based modeling, interactive, reactive and distributed real-time simulations, as well as responsive user applications, with a uniform model relying on the *k.Actors language* for behavior specification. _Actors_ that can receive a behavior include not only observations (agent-based modeling) but also sessions and users, where behaviors provide an _application framework_ for both interactive and non-interactive applications that can be coded quickly and concisely and run indifferently on a desktop or online. Behaviors specified in k.Actors are mapped to the http://akka.io[Akka] framework to create interactive, distributed simulations and applications.

ROUND-TRIP lifecycle of information between the layers. Live observations and dataflows can be saved to resources and curated as such, then reconnected to semantics (the same or not) and reused. Info from the resource layer is contextualized and ... After, both dynamic and static info can be moved back to the resource layer.

### The resource layer

The resource layer is where data and models provided by the community are published, curated and accessed. Resources consist of anything that can be linked to semantics: thus, not only datasets and databases but also model components, equations, entire computational services (such as a climate model runnable through an API) or, in the simplest cases, even single numbers. Each resource is identified by a unique, stable Uniform Resource Name (URN) which is assigned upon _publishing_, which also establishes a set of editable access permissions. In addition to the URN, each resource has a _type_ and a _geometry_. Interoperability at the resource level is guaranteed by _adapters_, software plugins that can be added to the system to bridge data formats or service APIs to the uniform k.LAB resource model. 

Lifecycle. Ensuring safety, documentation and validation.

LOD paradigm.

Main API calls in the resource layer

Search and discover API (no semantics: by metadata, author etc)

Access control

### The semantic layer

Two levels: the worldview (shared and synchronized, based on certificate) and models (annotation) where semantics is specified for resources and computational procedures (both curated from external sources or defined within k.LAB as components).

(ontologies rigorously ONLY about the meaning, never the representation, metadata or distribution)

Operations here are _resolution_ of the concept in the context (producing a self-consistent _dataflow_ capable of computing an observation of it) and _contextualization_ (the computation of the dataflow to produce the contextualized observation). Machine reasoning is the principal means to resolve the concept, using inference to find ways to compute the observation from the findable information in the k.LAB resource layer.

#### Concepts: the worldview

Semantics used how - e.g. redistribution of contexts (watersheds) and inherency (height of tree); automatic modelling; The k.IM language, in contrast with the common "one concept, one artifact" paradigm of other semantic annotation approaches, employs linguistic (English-like) constructs to compose logical expressions (_observables_) that preserve semantics for all their components and capture the inner structure of the observation process so that the AI engine can find the best strategy to compute their observations. ... EXAMPLES

#### Models

Besides the shared worldview, information in the semantic layer is created by _contextualizing concepts_, which produces _observations_ that live in a _session_ and are relative to a root observation (the _observation context_) chosen by the user and defining the overall spatial and temporal scale of reference (although each individual observation can provide a different scale, with automatic mediation when necessary). Importantly, observations may be static in the context (e.g. the observation of _qualities_, such as elevation) or *dynamic* (e.g. observations of _processes_, such as SurfaceWaterFlow): the word _observation_ is used here to mean the result of any contextualized concept, including those that imply change in time (processes, events, functional relationships); time can be simulated or real. In fact, each contextualized observation is in fact a software _agent_ whose _behavior_ in the semantic layer is estabilished upon resolution; the _dataflow_ built by the system after the user has (1) defined a context and (2) queried a concept to observe in it (the main operation in k.LAB) defines it.

The k.IM language used to specify the worldview also has the `model` construct that allowa.....

EXAMPLES OF MODELS 
PARSIMONY
MODULARITY


#### Learning models

Machine learning integrated in the 

#### Authorities

### The reactivity layer

The reactivity layer provides _behaviors_ to any of the agents living in the semantic layer. These include not only any observation created through contextualization, but also sessions, users, and the partners themselves. Behaviors are specified in the `k.Actors` language and can be bound explicitly or through bindings specified by models. 

This _reactive modelling_ is a novel extension that sees the products of a modelling session not only as knowledge for direct perusal or curation, but also as “live” agents that can react to events and talk to each other through the network – a sort of “internet of things” that connects scientific observations, sensors and models instead of appliances and devices. In the terms of the k.LAB technology, this is accomplished directly by binding the results of a k.LAB session (scientific observations “living” in a networked server) to a behavior that enables them to react to events, coming from users or from other observations. This enables building:
1.	Distributed simulations where authoritative sites provide state-of-the-art modelling of specific phenomena (say climate or human migration) and other researchers can simply connect to those contexts to simulate related observables in the most reliable incarnations (in simulated time or in real time). 
2.	A much easier definition of scenarios for predictive modeling, as changing the frame of reference for a simulation becomes a matter of switching the URL of a context with that of another, ideally provided by institutions with the relevant expertise.
3.	The ability of easily sketching and connect ad-hoc applications by using the same techniques to bind users and sessions to behaviors, which can implement specific bottom-lines and interfaces tailored to communities, languages and problem areas.
From a technical perspective, this innovation can be reached by using well-established actor system theory and by incorporating actor framework into the k.LAB software. A specialized language (k.Actors) can be used to specify behaviors in a very simple and intuitive way, building on the integration with the semantic language already used in k.LAB and on existing IOT technologies (Akka Actors). 

## Creating a k.LAB semantic network

## Client applications

### Content providers

### Content fruition

### Access and security

## Contributing knowledge and extension points

### Contributed knowledge

#### Worldview tiers

#### Resource content

#### Semantic content

### Extension points

#### Resource Adapters

... List of adapters existing and in development 

#### Engine components

## Applications

## State and future of the software

[bibliography]
== Resources

[bibliography]
.Software 
- [[[taoup]]] Eric Steven Raymond. 'The Art of Unix
  Programming'. Addison-Wesley. ISBN 0-13-142901-9.
- [[[walsh-muellner]]] Norman Walsh & Leonard Muellner.
  'DocBook - The Definitive Guide'. O'Reilly & Associates. 1999.
  ISBN 1-56592-580-7.
  
.Articles 
- [[[taoup]]] Eric Steven Raymond. 'The Art of Unix
  Programming'. Addison-Wesley. ISBN 0-13-142901-9.
- [[[walsh-muellner]]] Norman Walsh & Leonard Muellner.
  'DocBook - The Definitive Guide'. O'Reilly & Associates. 1999.
  ISBN 1-56592-580-7.
  
.Web sites 
- [[[taoup]]] Eric Steven Raymond. 'The Art of Unix
  Programming'. Addison-Wesley. ISBN 0-13-142901-9.
- [[[walsh-muellner]]] Norman Walsh & Leonard Muellner.
  'DocBook - The Definitive Guide'. O'Reilly & Associates. 1999.
  ISBN 1-56592-580-7.
  
.Application projects 
- [[[taoup]]] Eric Steven Raymond. 'The Art of Unix
  Programming'. Addison-Wesley. ISBN 0-13-142901-9.
- [[[walsh-muellner]]] Norman Walsh & Leonard Muellner.
  'DocBook - The Definitive Guide'. O'Reilly & Associates. 1999.
  ISBN 1-56592-580-7.


