[#chapter-overview]
= k.LAB: an infrastructure for integrated science
:doctype: book

////
see if preamble is OK as is or we want another. 
////

`k.LAB` supports *integrated modeling*, a scientific pathway that reconciles strong semantics with the production, use and curation of scientific artifacts such as datasets, models and computational services. Integrated modeling aims to enable a more efficient, valuable and _democratic_ science by providing actionable _modularity_, _interoperability_, _reusability_, and _openness_ through multiple paradigms and scales. 

To achieve this goal, k.LAB keeps the logical representation of the observed world distinct from the physical data or the functional knowledge that allows it to be described, and uses artificial intelligence to assemble computations that produce *observations* of such knowledge by finding, linking and connecting information made available by other scientists. Because information is mandatorily linked to known, stable and fully specified _semantics_, each scientist can work on their specific area of interest without concern for data representation, visualization, storage, or for any of the domains that are not of their specific interest, leaving all integration tasks to k.LAB.

For example, a scientist may have developed a method to compute the probability of a region to become flooded when sufficient precipitation is received (below we show the total wetness index, a known metric). The k.IM model for that method may look like this:

[source,kim]
----
model occurrence of earth:Region with im:Still earth:PrecipitationVolume <1>
	observing 
		geography:Slope in ° named slope, <2>
		hydrology:ContributingArea in m² named contributing_area <3>
	set to [ <4>
		def sloperad = Math.tan((slope * 1.570796 )/90)
		return Math.log((contributing_area+1)/Math.tan((sloperad+0.001)));
	];
----

Reads as English and is, to a good extent, self-documenting. Describe callouts: 

- Describe where the concepts come from (the distributed, self-updating <<worldview.adoc#chapter-worldview,worldview>>)
- Describe lack of context, automatic transformations and recontextualizations: to space, time and scale
- Describe units and spatialization
- Describe choice of data 

Show definition of concepts in worldview

Show how users can use it in the explorer

Show how this can be shared with others.

Show dataflow in Explorer

Example with time and agents

Example with machine learning

IMPORTANT: REVIEW ENTIRELY! 

Classes of knowledge and where they are stored

Knowledge/worldviews; observables -> shared and synchronized, collecting community thinking, constantly reviewed
Non-semantic data and computations; resources URNS, local -> public
Semantic models; k.IM projects, basically the content of the SWWW, usable by searching (like code above)

Lifecycle of each of the 3

In k.LAB, if the observable concepts are compatible, models for those observables also are. The semantics is the backbone of distributed computations and ensures that each investigator can develop only parts of models, guaranteeing the soundness of the results and allowing a scalable, adaptable computation strategy that adapts to 

EXAMPLES and USE CASES RIGHT HERE: 

- a researcher may have data for XXX, elevation - find concept and read the definition; if ok, annotate the data and hit publish. The file is uploaded and published to a geoserver (more on this later); filename becomes a URN. The world has the data.
- another may have found a method to compute YYY, the flood potential of an area depending on X and Y. Find concepts and read the definition; if ok, annotate the equation and try. WIthout any data. Works. Hit publish and now the world has the method.
- A user may just ask to observe the concept in a context using one of several interfaces (Java, Web, IDE) - example. 
- The same applies to any other :ref:`observables`: different observable produce different observations. for example a process (ideally: show a R model)... Complex model - show interface code. World has hydrological model. User may ask for runoff in a time period.
- Show picture of water model in IDE. 
- The user does not even know what a model is - all they share is a pointer to the worldview.
- The modeler writing the model only worries about XXX and not about YYY

In order for models to be compatible and be capable of being used as component of the same computation, it is sufficient that the abstract knowledge they represent is compatible. Modern artificial intelligence provides algorithms and tools to automatically validate the consistency of an abstract knowledge base. This way, the approach enables the integration of many modeling paradigms that are often applied separately, for example spatially-explicit to process- and agent-based models, or probabilistic and deterministic models. This conceptualization builds a natural path to modular modeling, multiple-paradigm modeling, multiple-scale modeling and structurally variable modeling.

The logical representation is modeled using concepts and relationships that comprise k.LAB's abstract knowledge base, built out of ontologies (more on this later). In the abstract knowledge base, concepts such as "watershed," "elevation," or "temperature" are defined, along with information on how they may relate to each other. No attempt is made to indicate how their models may be computed.

The algorithmic knowledge base is where you can provide models so that simulated "observations" can be computed. At the user's request, the artificial intelligence in k.LAB will choose algorithms from this knowledge base and build an integrated algorithm by assembling them, driven by the abstract semantics. The result of calibrating and running the integrated model is the production of observations of the concepts contained in the abstract knowledge base.

A model in k.LAB represents a _strategy to observe a concept_. Models can consist of entire complex simulations, simpler functions, look-up tables or classifications, datasets, or even single numbers; from the k.LAB point of view, all these just represent different ways to observe a concept. In striking difference from almost all mainstream modeling approaches, numbers, data or equations have absolutely no meaning by themselves, even with descriptive names or associated with formal metadata: even the simplest number can only be used in k.LAB if it has a concept associated with it. k.LAB forces you to use concepts so that "metadata" in your models only document auxiliary information as they should; the conceptual part of the knowledge base serves automatically as the documentation of the models, while at the same time providing a layer of "meaning" on which collaboration and model integration are based.


LOOK OUT FOR:

	Differences with other frameworks we may be used to
		Models are global by default and semantics is not scoped! Risks 
		of using same concepts and "playing around" with models.
		 
