= The k.LAB infrastructure for integrated science
:doctype: book

== Overview

IMPORTANT: REVIEW ENTIRELY!

`k.LAB` aims to address the task of "integrated modeling", which reconciles strong semantics with modeling practice, helping achieve advantages such as modularity, interoperability, reusability, and integration of multiple paradigms and scales. To achieve this goal, k.LAB keeps the logical representation of the modeled world distinct from the algorithmic knowledge that allows it to be simulated, and uses artificial intelligence to assemble computations that produce *observations* of such knowledge.

In this document, the term *observation* identifies an informational artifact that describes an *observable concept*, PUT A TABLE HERE such as the elevation of a region of Earth (the artifact is a map), the length of a train (a number), or the way water flows on a landscape (a set of differential equations). If science produces observations of the physical world, a *model* is a procedure that *defines* observations, i.e. it represents a "strategy" to produce observations of an observable concept. k.LAB is a software platform that supports:
- compiling observable concepts for many domains of knowledge into a single :ref:`worldview <worldview>`, composed of an extensible set of *ontologies*, which can be shared among users and network endpoints;
- compiling and storing models describing the worldview's concepts in independent network nodes; 
- assist users in creating *observations* of concepts by automatically choosing and assembling computations to build them, automatically negotiating links between data and models and assembling more complex models out of simpler ones.

In k.LAB, if the observable concepts are compatible, models for those observables also are. The semantics is the backbone of distributed computations and ensures that each investigator can develop only parts of models, guaranteeing the soundness of the results and allowing a scalable, adaptable computation strategy that adapts to 

EXAMPLES and USE CASES RIGHT HERE: 

- a researcher may have data for XXX, elevation - find concept and read the definition; if ok, annotate the data and hit publish. The file is uploaded and published to a geoserver (more on this later); filename becomes a URN. The world has the data.
- another may have found a method to compute YYY, the flood potential of an area depending on X and Y. Find concepts and read the definition; if ok, annotate the equation and try. WIthout any data. Works. Hit publish and now the world has the method.
- A user may just ask to observe the concept in a context using one of several interfaces (Java, Web, IDE) - example. 
- The same applies to any other :ref:`observables`: different observable produce different observations. for example a process (ideally: show a R model)... Complex model - show interface code. World has hydrological model. User may ask for runoff in a time period.
- Show picture of water model in IDE. 
- The user does not even know what a model is - all they share is a pointer to the worldview.
- The modeler writing the model only worries about XXX and not about YYY

In order for models to be compatible and be capable of being used as component of the same computation, it is sufficient that the abstract knowledge they represent is compatible. Modern artificial intelligence provides algorithms and tools to automatically validate the consistency of an abstract knowledge base. This way, the approach enables the integration of many modeling paradigms that are often applied separately, for example spatially-explicit to process- and agent-based models, or probabilistic and deterministic models. This conceptualization builds a natural path to modular modeling, multiple-paradigm modeling, multiple-scale modeling and structurally variable modeling.

The logical representation is modeled using concepts and relationships that comprise k.LAB's abstract knowledge base, built out of ontologies (more on this later). In the abstract knowledge base, concepts such as "watershed," "elevation," or "temperature" are defined, along with information on how they may relate to each other. No attempt is made to indicate how their models may be computed.

The algorithmic knowledge base is where you can provide models so that simulated "observations" can be computed. At the user's request, the artificial intelligence in k.LAB will choose algorithms from this knowledge base and build an integrated algorithm by assembling them, driven by the abstract semantics. The result of calibrating and running the integrated model is the production of observations of the concepts contained in the abstract knowledge base.

A model in k.LAB represents a strategy to observe a concept. Models can consist of entire complex simulations, simpler functions, look-up tables or classifications, datasets, or even single numbers; from the k.LAB point of view, all these just represent different ways to observe a concept. In striking difference from almost all mainstream modeling approaches, numbers, data or equations have absolutely no meaning by themselves, even with descriptive names or associated with formal metadata: even the simplest number can only be used in k.LAB if it has a concept associated with it. k.LAB forces you to use concepts so that "metadata" in your models only document auxiliary information as they should; the conceptual part of the knowledge base serves automatically as the documentation of the models, while at the same time providing a layer of "meaning" on which collaboration and model integration are based.


LOOK OUT FOR:

	Differences with other frameworks we may be used to
		Models are global by default and semantics is not scoped! Risks 
		of using same concepts and "playing around" with models.
		 
Mixed content
